{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e56cb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786d457d",
   "metadata": {},
   "source": [
    "<h1><strong><u>LSTM Text Character Generator Model</u></strong></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import io\n",
    "import keras\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras import Input, activations\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import SimpleRNN, Dense, LSTM, Dropout, Embedding\n",
    "from keras.losses import CategoricalCrossentropy, BinaryCrossentropy\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a52a73b",
   "metadata": {},
   "source": [
    "<h2><strong><u>Data Preparation</u></strong></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c889dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get text from link\n",
    "path = keras.utils.get_file(\n",
    "    \"nietzsche.txt\",\n",
    "    origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc73de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 600893\n",
      "Tokenized text length: 99111\n"
     ]
    }
   ],
   "source": [
    "#Open text to read and process it\n",
    "with io.open(path, encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "    text_tokenized = text.split()\n",
    "print(\"Corpus length:\", len(text))\n",
    "print(\"Tokenized text length:\", len(text_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bd0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct words and punctuation marks: 17682\n"
     ]
    }
   ],
   "source": [
    "# Create distinct list of word instances\n",
    "distinct_words = sorted(list(set(text_tokenized)))\n",
    "print(\"Total distinct words and punctuation marks:\", len(distinct_words))\n",
    "\n",
    "#Create a dictionary/mapping of the word instance to numbers\n",
    "# and numbers to word instances\n",
    "word_indices = dict((word, i) for i, word in enumerate(distinct_words))\n",
    "indices_word = dict((i, word) for i, word in enumerate(distinct_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26188c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 49546\n"
     ]
    }
   ],
   "source": [
    "# cut text in partially redundant seequences of maxlen words\n",
    "maxlen = 20\n",
    "steps = 2\n",
    "sequences = []\n",
    "next_words = []\n",
    "for i in range(0, len(text_tokenized) - maxlen, steps):\n",
    "    sequences.append(text_tokenized[i : i + maxlen])\n",
    "    next_words.append(text_tokenized[i + maxlen])\n",
    "print(\"Number of sequences:\", len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb259e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make feature and target tensors from sequences and next_words list objects\n",
    "X = np.zeros((len(sequences), maxlen, len(distinct_words)), dtype='bool')\n",
    "y = np.zeros((len(sequences), len(distinct_words)), dtype='bool')\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, word in enumerate(sequence):\n",
    "        X[i, t, word_indices[word]] = 1\n",
    "    y[i, word_indices[next_words[i]]] = 1\n",
    "\n",
    "# print(\"Debug\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86bfd6",
   "metadata": {},
   "source": [
    "<h2><strong><u>LSTM Model Selection</u></strong></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(distinct_words))),\n",
    "        LSTM(128),\n",
    "        Dense(len(distinct_words), activation=\"softmax\")\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6aca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=0.5):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 128\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated = \"\"\n",
    "sequence = text[start_index : start_index + maxlen]\n",
    "print('...Generating with seed:', *sequence)\n",
    "\n",
    "model.fit(X, y, batch_size=batch_size, epochs=1)\n",
    "\n",
    "for i in range(400):\n",
    "    x_pred = np.zeros((1, len(sequence), len(word)))\n",
    "    for t, char in enumerate(sequence):\n",
    "        x_pred[0, t, word_indices[char]] = 1.0\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds)\n",
    "    next_word = indices_word[next_index]\n",
    "    sequence.append(next_word)\n",
    "\n",
    "print(\"...Generated: \", *sequence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
